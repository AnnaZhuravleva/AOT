{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "m = MorphAnalyzer()\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "import RAKE\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('russian')\n",
    "rake = RAKE.Rake(stop)\n",
    "from summa import keywords\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.summarization import keywords as kw\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = tokenizer.tokenize(text.lower())\n",
    "    lemmas = [m.parse(t)[0].normal_form for t in text]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    text = tokenizer.tokenize(text.lower())\n",
    "    return [m.parse(t)[0].normal_form for t in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['индивидуальный',\n",
       "  'различие',\n",
       "  'характеристика',\n",
       "  'признак',\n",
       "  'психология',\n",
       "  'причина',\n",
       "  'личность'],\n",
       " ['психологический',\n",
       "  'тип',\n",
       "  'типология',\n",
       "  'характеристика',\n",
       "  'различие',\n",
       "  'группа',\n",
       "  'характер'],\n",
       " ['черта',\n",
       "  'психологический',\n",
       "  'характеристика',\n",
       "  'отличие',\n",
       "  'концептуализация',\n",
       "  'свойство',\n",
       "  'классификация'],\n",
       " ['интеллект',\n",
       "  'исследование',\n",
       "  'тип',\n",
       "  'показатель',\n",
       "  'информация',\n",
       "  'память',\n",
       "  'характеристика'],\n",
       " ['личность',\n",
       "  'темперамент',\n",
       "  'эмоциональность',\n",
       "  'поведение',\n",
       "  'характеристика',\n",
       "  'тип',\n",
       "  'характер']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./texts/meta.txt', 'r', encoding='utf-8') as f:\n",
    "    meta = [normalize(line) for line in f.read().split('\\n')]\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for file in os.listdir('./texts'):\n",
    "    if file.startswith('text'):\n",
    "        path = os.path.join('./texts/', file)\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = int(file[4])\n",
    "            data.append(tuple([f.read(), meta[nb-1]]))\n",
    "\n",
    "corpus = [d[0] for d in data]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for file in os.listdir('./texts'):\n",
    "    if file.startswith('text'):\n",
    "        path = os.path.join('./texts/', file)\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data.append(tuple([f.read(), meta[nb-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textRank(text):\n",
    "    text = normalize_text(text)\n",
    "    result = []\n",
    "    kw(text, pos_filter=[], scores=True)\n",
    "    keywords.keywords(normalize_text(text), additional_stopwords=stop, scores=True)\n",
    "    G = keywords.get_graph(text)\n",
    "    result = [(edge, G.edge_weight(edge)) for edge in G.edges() if all(map(lambda x: x not in stop, edge))]\n",
    "    result = sorted(result, key = lambda x: x[1], reverse=True)[:7]\n",
    "    for i in result:\n",
    "        print(i)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " def TfIdf(text, ng = (1,3)):\n",
    "    vectorizer = TfidfVectorizer(ngram_range = ng)\n",
    "    X = vectorizer.fit_transform([normalize_text(text)])\n",
    "    names = vectorizer.get_feature_names()\n",
    "    words = {}\n",
    "    for col in X.nonzero()[1]:\n",
    "        if names[col] not in stop:\n",
    "            words[names[col]] =  float(X[0, col])\n",
    "    top = sorted(words.items(), key = lambda x: x[1], reverse=True)\n",
    "    for i in top[:5]:\n",
    "        print(i[0], i[1])\n",
    "    return [i[0] for i in top[:7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rake(text):\n",
    "    pred = []\n",
    "    for i in rake.run(normalize_text(text), minFrequency=2, maxWords=3)[:7]:\n",
    "            print(i[0], i[1])\n",
    "            pred.append(i[0])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textrank_res = []\n",
    "for d in corpus:\n",
    "    textrank_res.append(textRank(d))\n",
    "    print('============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_res = []\n",
    "for d in corpus:\n",
    "    tfidf_res.append(TfIdf(d, ng = (3,3)))\n",
    "    print('==================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in corpus:\n",
    "    TfIdf(d, ng = (1,3))\n",
    "    print('==================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_res = []\n",
    "for d in corpus:\n",
    "    tfidf_res.append(TfIdf(d, ng = (2,2)))\n",
    "    print('==================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rake_res = []\n",
    "for d in data:\n",
    "    rake_res.append(Rake(d[0]))\n",
    "    print('==================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_res, '\\n', textrank_res, '\\n', rake_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "metrics_true = []\n",
    "y_pred = []\n",
    "for line in meta:\n",
    "    metrics_true.append([1] * len(line))\n",
    "    y_pred.append([0] * len(line))\n",
    "(metrics_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rake_pred = deepcopy(y_pred)\n",
    "tfidf_pred = deepcopy(y_pred)\n",
    "textrank_pred = deepcopy(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nb,text in enumerate(tfidf_res):\n",
    "    print(nb, text, meta[nb])\n",
    "    for idx, words in enumerate(text):\n",
    "        line = words.split()\n",
    "        tfidf_pred[nb][idx] = int(True in (map(lambda x: x in meta[nb], line)))\n",
    "tfidf_pred            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [item for inner in metrics_true for item in inner]\n",
    "y_pred = [item for inner in tfidf_pred for item in inner]\n",
    "# sklearn.metrics.accuracy_score(y_true, y_pred), sklearn.metrics.recall_score(y_true, y_pred), f1_score(y_true, y_pred) \n",
    "m = sklearn.metrics.precision_recall_fscore_support(y_true, y_pred)\n",
    "print('\\t'.join([str(item) for item in m]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ошибки\n",
    "\n",
    "* Не выкидывает местоимения (какой-то), нужно их включать в стоп-слова\n",
    "* Самые частотные != ключевые"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nb,text in enumerate(rake_res):\n",
    "    print(nb, text, meta[nb])\n",
    "    for idx, words in enumerate(text):\n",
    "        line = words.split()\n",
    "        rake_pred[nb][idx] = int(True in (map(lambda x: x in meta[nb], line)))\n",
    "rake_pred  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAKE\n",
    "y_pred = [item for inner in rake_pred for item in inner]\n",
    "\n",
    "m = sklearn.metrics.precision_recall_fscore_support(y_true, y_pred)\n",
    "print('\\t'.join([str(item) for item in m]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ошибки\n",
    "\n",
    "* Лишние слова, которые берет во внимание - \"который\", \"е\"\n",
    "* В биграммах и просто словах - повторение одних и тех же слов (\"выше интеллект\" и \"интеллект\")\n",
    "* Самыйй частотные слова != ключевые"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nb,text in enumerate(textrank_res):\n",
    "    for idx, words in enumerate(text):\n",
    "        textrank_pred[nb][idx] = int(True in (map(lambda x: x in meta[nb], words)))\n",
    "textrank_pred  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_rank\n",
    "y_pred = [item for inner in textrank_pred for item in inner]\n",
    "# sklearn.metrics.accuracy_score(y_true, y_pred), sklearn.metrics.recall_score(y_true, y_pred), f1_score(y_true, y_pred) \\\n",
    "m = sklearn.metrics.precision_recall_fscore_support(y_true, y_pred)\n",
    "print('\\t'.join([str(item) for item in m]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ошибки:\n",
    "\n",
    "* Считает биграммы подряд, в том порядке, в котором они в тексте\n",
    "* В данном случае для всех биграмм частота равна 1, т.е. они встретилсь в тексте один раз, что сомнительно\n",
    "* В итоговом результате - ни одного пересечения с ключевыми словами\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итог\n",
    "\n",
    "* Для поиска ключевых слов недостаточно знать частотность\n",
    "* В исходных текстах встречалось много примеров описывамых явлений, поэтому частостность ключевых слов уменьшалась засчет преобладания других слов\n",
    "* Системы не всегда считают частоты биграмм корректно, включая повторы слов или лишние слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
